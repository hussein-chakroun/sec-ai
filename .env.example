# LLM Configuration
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Choose LLM provider: openai, anthropic, lmstudio, ollama
LLM_PROVIDER=openai
LLM_MODEL=gpt-4-turbo-preview

# LM Studio Configuration (Local LLM via LM Studio)
# Download LM Studio: https://lmstudio.ai/
# 1. Download and install LM Studio
# 2. Download a model (e.g., Mistral 7B, Llama 2, etc.)
# 3. Start the local server in LM Studio (default port 1234)
# 4. Set LLM_PROVIDER=lmstudio
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model
# Or use the actual model filename loaded in LM Studio:
# LMSTUDIO_MODEL=mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Ollama Configuration (Local LLM via Ollama)
# Install Ollama: https://ollama.ai/
# 1. Install Ollama: curl https://ollama.ai/install.sh | sh
# 2. Pull a model: ollama pull llama2
# 3. Start Ollama: ollama serve (runs on port 11434 by default)
# 4. Set LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
# Other popular models: mistral, codellama, neural-chat, vicuna

# Application Settings
LOG_LEVEL=INFO
MAX_SCAN_THREADS=5

# Tool Paths (leave empty for system PATH)
NMAP_PATH=
SQLMAP_PATH=
HYDRA_PATH=
METASPLOIT_PATH=

# Report Settings
REPORT_OUTPUT_DIR=./reports_output
