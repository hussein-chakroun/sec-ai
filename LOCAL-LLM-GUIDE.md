# Using Local LLMs with LM Studio / Ollama

This guide shows you how to use locally-hosted LLMs instead of cloud APIs (OpenAI/Anthropic).

## Benefits of Local LLMs

‚úÖ **Privacy**: All data stays on your machine
‚úÖ **Cost**: No API fees - free after initial setup
‚úÖ **Offline**: Works without internet connection
‚úÖ **Control**: Use custom or fine-tuned models
‚úÖ **Speed**: No API latency for smaller models

## Option 1: LM Studio (Recommended for Windows/Mac)

### Setup

1. **Download LM Studio**
   - Visit https://lmstudio.ai/
   - Download for your OS (Windows/Mac/Linux)
   - Install the application

2. **Download a Model**
   - Open LM Studio
   - Go to "Search" tab
   - Recommended models for pentesting:
     - **Mistral 7B Instruct** (best balance) - `TheBloke/Mistral-7B-Instruct-v0.2-GGUF`
     - **Llama 2 13B** (more capable) - `TheBloke/Llama-2-13B-chat-GGUF`
     - **Neural Chat 7B** (good for technical tasks) - `TheBloke/neural-chat-7B-v3-1-GGUF`
   - Download the Q4_K_M quantization (good speed/quality balance)

3. **Start the Local Server**
   - Click on "Local Server" tab in LM Studio
   - Select your downloaded model
   - Click "Start Server"
   - Server will start on `http://localhost:1234` by default
   - ‚úÖ Green indicator = server is running

4. **Configure sec-ai**
   ```bash
   # Edit .env file
   LLM_PROVIDER=lmstudio
   LMSTUDIO_BASE_URL=http://localhost:1234/v1
   LMSTUDIO_MODEL=local-model
   ```

5. **Run sec-ai**
   ```bash
   python main.py --target example.com
   ```

### Recommended Models for Pentesting

| Model | Size | RAM Required | Best For |
|-------|------|--------------|----------|
| Mistral 7B Instruct Q4 | 4.1 GB | 8 GB | General pentesting, fast responses |
| Llama 2 13B Chat Q4 | 7.4 GB | 16 GB | Complex reasoning, better exploit chains |
| CodeLlama 13B Q4 | 7.4 GB | 16 GB | Code analysis, script generation |
| Neural Chat 7B Q4 | 4.1 GB | 8 GB | Technical analysis |

### Performance Tips

- **Quantization**: Q4_K_M is best balance. Q5 for better quality, Q3 for speed
- **Context Length**: Set to 4096+ for longer conversations
- **GPU Acceleration**: Enable in LM Studio settings for 10-100x speedup
- **Temperature**: 0.7 for pentesting (balance creativity and precision)

---

## Option 2: Ollama (Recommended for Linux/CLI)

### Setup

1. **Install Ollama**
   ```bash
   # Linux
   curl https://ollama.ai/install.sh | sh
   
   # macOS
   brew install ollama
   
   # Windows
   # Download from https://ollama.ai/download/windows
   ```

2. **Pull a Model**
   ```bash
   # Recommended for pentesting
   ollama pull mistral        # 4 GB, fast and capable
   ollama pull llama2         # 4 GB, good reasoning
   ollama pull codellama      # 4 GB, best for code/scripts
   ollama pull neural-chat    # 4 GB, technical tasks
   ```

3. **Start Ollama Server**
   ```bash
   ollama serve
   # Server runs on http://localhost:11434
   ```

4. **Configure sec-ai**
   ```bash
   # Edit .env file
   LLM_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=mistral
   ```

5. **Run sec-ai**
   ```bash
   python main.py --target example.com
   ```

### Ollama Commands

```bash
# List available models
ollama list

# Pull new model
ollama pull <model-name>

# Run model interactively (test)
ollama run mistral

# Delete model
ollama rm <model-name>

# Show model info
ollama show mistral
```

---

## Comparison: LM Studio vs Ollama

| Feature | LM Studio | Ollama |
|---------|-----------|--------|
| **Interface** | GUI (user-friendly) | CLI (lightweight) |
| **OS Support** | Windows, Mac, Linux | Linux, Mac, Windows |
| **Model Management** | Visual model browser | Command-line pull |
| **Server Management** | GUI start/stop | Background service |
| **Resource Usage** | Slightly higher | Very lightweight |
| **Best For** | Desktop users, beginners | Servers, advanced users |

---

## Quick Start Examples

### Example 1: Basic Pentest with LM Studio
```bash
# 1. Start LM Studio server with Mistral 7B
# 2. Configure
echo "LLM_PROVIDER=lmstudio" > .env
echo "LMSTUDIO_MODEL=local-model" >> .env

# 3. Run
python main.py --target 192.168.1.100
```

### Example 2: Multi-Agent Swarm with Ollama
```python
from core.ultimate_engine import UltimatePentestEngine
from core.config import load_config

# Configure for Ollama
import os
os.environ['LLM_PROVIDER'] = 'ollama'
os.environ['OLLAMA_MODEL'] = 'mistral'

# Initialize
config = load_config()
engine = UltimatePentestEngine(config)

# Run with local LLM
result = await engine.run_ultimate_pentest(
    target="example.com",
    engagement_type="comprehensive",
    stealth_mode="high"
)
```

### Example 3: Testing Local LLM Connection
```python
from core.llm_orchestrator import LMStudioProvider

# Test LM Studio
provider = LMStudioProvider(
    base_url="http://localhost:1234/v1",
    model="local-model"
)

response = provider.generate(
    "List the top 5 web application vulnerabilities",
    system_prompt="You are a penetration tester."
)
print(response)
```

---

## Troubleshooting

### LM Studio Issues

**Problem**: "Connection refused" error
- ‚úÖ Make sure LM Studio server is running (green indicator)
- ‚úÖ Check the port (default 1234)
- ‚úÖ Verify base URL: `http://localhost:1234/v1`

**Problem**: Very slow responses
- ‚úÖ Enable GPU acceleration in LM Studio settings
- ‚úÖ Try smaller model (Mistral 7B instead of 13B)
- ‚úÖ Use Q4 quantization instead of Q5/Q6

**Problem**: Model not loading
- ‚úÖ Check available RAM (need 2x model size)
- ‚úÖ Close other applications
- ‚úÖ Try restarting LM Studio

### Ollama Issues

**Problem**: "ollama: command not found"
- ‚úÖ Reinstall Ollama
- ‚úÖ Add to PATH: `export PATH=$PATH:/usr/local/bin`

**Problem**: Server not starting
- ‚úÖ Check if already running: `ps aux | grep ollama`
- ‚úÖ Kill existing: `pkill ollama`
- ‚úÖ Restart: `ollama serve`

**Problem**: Model giving poor results
- ‚úÖ Try different model: `ollama pull mistral`
- ‚úÖ Increase context length in prompts
- ‚úÖ Use more specific system prompts

---

## Performance Benchmarks

Tested on: Intel i7-10700K, 32GB RAM, RTX 3080

| Model | Provider | Tokens/sec | Quality | RAM Usage |
|-------|----------|------------|---------|-----------|
| GPT-4 Turbo | OpenAI | ~50 (API) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Cloud |
| Claude 3 Opus | Anthropic | ~40 (API) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Cloud |
| Mistral 7B Q4 | LM Studio | 45 | ‚≠ê‚≠ê‚≠ê‚≠ê | 6 GB |
| Llama 2 13B Q4 | LM Studio | 25 | ‚≠ê‚≠ê‚≠ê‚≠ê | 10 GB |
| Mistral 7B | Ollama | 42 | ‚≠ê‚≠ê‚≠ê‚≠ê | 6 GB |

*With GPU acceleration. CPU-only is ~10x slower*

---

## Recommended Hardware

### Minimum
- **CPU**: 4+ cores
- **RAM**: 16 GB
- **Storage**: 10 GB free
- **GPU**: Optional (CPU works but slower)

### Recommended
- **CPU**: 8+ cores
- **RAM**: 32 GB
- **Storage**: 50 GB free (for multiple models)
- **GPU**: NVIDIA GPU with 6+ GB VRAM (10-100x faster)

### Optimal
- **CPU**: 12+ cores
- **RAM**: 64 GB
- **Storage**: 100 GB SSD
- **GPU**: NVIDIA RTX 3080+ (12+ GB VRAM)

---

## Cost Savings

### Cloud API Costs (Estimated)
- GPT-4 Turbo: $0.01 - $0.03 per request
- Claude 3 Opus: $0.015 - $0.075 per request
- **1000 requests**: $10 - $75

### Local LLM Costs
- **Setup**: Free (LM Studio/Ollama)
- **Model**: Free (open source)
- **Runtime**: Only electricity (~$0.10/hour)
- **1000 requests**: ~$1 in electricity

**Savings**: 90-99% cost reduction after initial setup!

---

## Privacy & Security

### Cloud APIs
- ‚ùå Data sent to third-party servers
- ‚ùå Potential data retention
- ‚ùå Requires internet connection
- ‚ùå Subject to terms of service

### Local LLMs
- ‚úÖ All data stays on your machine
- ‚úÖ No external communication
- ‚úÖ Works offline
- ‚úÖ Full control over data
- ‚úÖ **Perfect for sensitive pentests**

---

## Next Steps

1. **Install LM Studio or Ollama** (5 minutes)
2. **Download a model** (10-30 minutes depending on internet)
3. **Update .env file** (1 minute)
4. **Run sec-ai** (instant)

**You're now running a fully autonomous pentesting platform with local LLMs!** üöÄ

For questions or issues, check the main documentation or open an issue on GitHub.
