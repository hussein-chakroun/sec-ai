"""
Reinforcement Learning Exploiter
Q-learning and neural network-based exploitation strategies
"""

import numpy as np
import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, deque
import random
import json


logger = logging.getLogger(__name__)


class QLearningAgent:
    """Q-Learning agent for exploitation path optimization"""
    
    def __init__(self, learning_rate: float = 0.1, discount_factor: float = 0.95, epsilon: float = 0.1):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.episode_rewards = []
        
    def get_action(self, state: str, available_actions: List[str]) -> str:
        """Select action using epsilon-greedy policy"""
        if random.random() < self.epsilon:
            return random.choice(available_actions)
        
        q_values = {action: self.q_table[state][action] for action in available_actions}
        max_q = max(q_values.values()) if q_values else 0
        best_actions = [action for action, q in q_values.items() if q == max_q]
        return random.choice(best_actions) if best_actions else random.choice(available_actions)
    
    def update(self, state: str, action: str, reward: float, next_state: str, next_actions: List[str]):
        """Update Q-table using Q-learning formula"""
        current_q = self.q_table[state][action]
        
        if next_actions:
            max_next_q = max([self.q_table[next_state][a] for a in next_actions])
        else:
            max_next_q = 0
        
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state][action] = new_q
        
    def decay_epsilon(self, decay_rate: float = 0.995):
        """Decay exploration rate"""
        self.epsilon = max(0.01, self.epsilon * decay_rate)


class NeuralExploitStrategy:
    """Neural network-based exploitation strategy learner"""
    
    def __init__(self, architecture: str = 'lstm'):
        self.architecture = architecture
        self.model = None
        self.training_history = []
        
    def build_model(self, input_dim: int, output_dim: int):
        """Build neural network model for strategy prediction"""
        # Simulated model - in production, use TensorFlow/PyTorch
        self.model = {
            'type': self.architecture,
            'input_dim': input_dim,
            'output_dim': output_dim,
            'weights': np.random.randn(input_dim, output_dim) * 0.01
        }
        
    def train(self, training_data: List[Dict[str, Any]], epochs: int = 100):
        """Train neural network on successful attack patterns"""
        logger.info(f"Training {self.architecture} model for {epochs} epochs")
        
        for epoch in range(epochs):
            total_loss = 0
            for sample in training_data:
                # Simulated training
                loss = random.random() * (1 - epoch / epochs)
                total_loss += loss
            
            avg_loss = total_loss / max(len(training_data), 1)
            self.training_history.append({
                'epoch': epoch,
                'loss': avg_loss
            })
            
        return self.training_history
    
    def predict(self, state_features: np.ndarray) -> np.ndarray:
        """Predict optimal action from state features"""
        if self.model is None:
            return np.random.rand(10)
        
        # Simulated prediction
        return np.random.rand(self.model['output_dim'])


class RLExploiter:
    """
    Reinforcement Learning-based Exploitation Engine
    
    Uses Q-learning and neural networks to optimize exploitation strategies
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.q_agent = None
        self.nn_strategy = None
        
        # Exploitation state space
        self.states = [
            'reconnaissance', 'scanning', 'exploitation', 'post_exploitation',
            'privilege_escalation', 'lateral_movement', 'data_exfiltration'
        ]
        
        # Action space
        self.actions = {
            'reconnaissance': ['port_scan', 'service_enum', 'web_crawl', 'dns_enum'],
            'scanning': ['vulnerability_scan', 'version_detection', 'os_fingerprint'],
            'exploitation': ['buffer_overflow', 'sql_injection', 'xss', 'rce', 'file_upload'],
            'post_exploitation': ['shell_stabilize', 'credential_dump', 'persistence'],
            'privilege_escalation': ['kernel_exploit', 'suid_abuse', 'sudo_exploit'],
            'lateral_movement': ['pass_the_hash', 'ssh_pivot', 'rdp_jump'],
            'data_exfiltration': ['database_dump', 'file_steal', 'network_transfer']
        }
        
    async def q_learning_exploit(
        self,
        target: str,
        episodes: int = 1000,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95
    ) -> List[Dict[str, Any]]:
        """
        Use Q-learning to find optimal exploitation paths
        """
        self.logger.info(f"Starting Q-learning exploitation with {episodes} episodes")
        
        self.q_agent = QLearningAgent(learning_rate, discount_factor, epsilon=0.3)
        results = []
        
        for episode in range(episodes):
            state = 'reconnaissance'
            episode_reward = 0
            episode_path = []
            steps = 0
            max_steps = 20
            
            while state != 'data_exfiltration' and steps < max_steps:
                # Get available actions for current state
                available_actions = self.actions.get(state, ['terminate'])
                
                # Select action
                action = self.q_agent.get_action(state, available_actions)
                
                # Simulate action execution and get reward
                next_state, reward, done = await self._simulate_action(target, state, action)
                
                episode_path.append({
                    'state': state,
                    'action': action,
                    'reward': reward,
                    'next_state': next_state
                })
                
                # Update Q-table
                next_actions = self.actions.get(next_state, [])
                self.q_agent.update(state, action, reward, next_state, next_actions)
                
                episode_reward += reward
                state = next_state
                steps += 1
                
                if done:
                    break
            
            self.q_agent.decay_epsilon()
            self.q_agent.episode_rewards.append(episode_reward)
            
            # Record successful episodes
            if episode_reward > 5.0:
                results.append({
                    'episode': episode,
                    'reward': episode_reward,
                    'path': episode_path,
                    'success': True,
                    'steps': steps
                })
        
        # Extract best exploitation path
        best_episode = max(results, key=lambda x: x['reward']) if results else None
        
        summary = {
            'total_episodes': episodes,
            'successful_episodes': len(results),
            'best_reward': best_episode['reward'] if best_episode else 0,
            'best_path': best_episode['path'] if best_episode else [],
            'q_table_size': len(self.q_agent.q_table),
            'convergence': self._check_convergence(self.q_agent.episode_rewards)
        }
        
        self.logger.info(f"Q-learning completed: {len(results)} successful paths found")
        
        return [summary] + results[:10]  # Return summary + top 10 results
    
    async def neural_network_strategies(
        self,
        target: str,
        training_data: List[Dict[str, Any]],
        architecture: str = 'lstm'
    ) -> List[Dict[str, Any]]:
        """
        Train neural networks on successful attack patterns
        """
        self.logger.info(f"Training {architecture} network on attack patterns")
        
        self.nn_strategy = NeuralExploitStrategy(architecture)
        
        # Prepare training data
        if not training_data:
            training_data = await self._generate_synthetic_training_data(target)
        
        # Build and train model
        self.nn_strategy.build_model(input_dim=20, output_dim=10)
        training_history = self.nn_strategy.train(training_data, epochs=100)
        
        # Test trained model
        test_results = []
        for i in range(10):
            state_features = np.random.rand(20)
            predicted_actions = self.nn_strategy.predict(state_features)
            
            test_results.append({
                'test_case': i,
                'state_features': state_features.tolist()[:5],  # First 5 features
                'predicted_actions': predicted_actions.tolist()[:5],  # Top 5 actions
                'confidence': float(np.max(predicted_actions))
            })
        
        return [{
            'architecture': architecture,
            'training_samples': len(training_data),
            'training_epochs': len(training_history),
            'final_loss': training_history[-1]['loss'] if training_history else 0,
            'test_results': test_results
        }]
    
    async def adaptive_strategies(
        self,
        target: str,
        reward_threshold: float = 0.8,
        exploration_rate: float = 0.2
    ) -> List[Dict[str, Any]]:
        """
        Dynamically adjust exploitation strategies based on rewards
        """
        self.logger.info("Applying adaptive strategy adjustment")
        
        strategies = [
            {'name': 'aggressive', 'speed': 'fast', 'stealth': 'low'},
            {'name': 'balanced', 'speed': 'medium', 'stealth': 'medium'},
            {'name': 'stealthy', 'speed': 'slow', 'stealth': 'high'}
        ]
        
        results = []
        strategy_rewards = defaultdict(list)
        
        for iteration in range(50):
            # Select strategy (epsilon-greedy)
            if random.random() < exploration_rate:
                strategy = random.choice(strategies)
            else:
                # Exploit best strategy
                avg_rewards = {s['name']: np.mean(strategy_rewards[s['name']]) if strategy_rewards[s['name']] else 0 
                              for s in strategies}
                best_strategy_name = max(avg_rewards, key=avg_rewards.get)
                strategy = next(s for s in strategies if s['name'] == best_strategy_name)
            
            # Execute strategy and get reward
            reward = await self._execute_strategy(target, strategy)
            strategy_rewards[strategy['name']].append(reward)
            
            results.append({
                'iteration': iteration,
                'strategy': strategy['name'],
                'reward': reward,
                'adapted': reward < reward_threshold
            })
        
        # Summary
        summary = {
            'total_iterations': len(results),
            'strategy_performance': {
                name: {
                    'avg_reward': np.mean(rewards),
                    'max_reward': np.max(rewards),
                    'executions': len(rewards)
                }
                for name, rewards in strategy_rewards.items()
            },
            'best_strategy': max(strategy_rewards.keys(), 
                                key=lambda k: np.mean(strategy_rewards[k]))
        }
        
        return [summary] + results[:10]
    
    async def _simulate_action(
        self,
        target: str,
        state: str,
        action: str
    ) -> Tuple[str, float, bool]:
        """Simulate action execution and return next state, reward, done"""
        await asyncio.sleep(0.001)  # Simulate work
        
        # Define state transitions
        transitions = {
            'reconnaissance': 'scanning',
            'scanning': 'exploitation',
            'exploitation': 'post_exploitation',
            'post_exploitation': random.choice(['privilege_escalation', 'lateral_movement']),
            'privilege_escalation': 'lateral_movement',
            'lateral_movement': 'data_exfiltration',
            'data_exfiltration': 'data_exfiltration'
        }
        
        next_state = transitions.get(state, state)
        
        # Calculate reward based on action success probability
        base_reward = random.random()
        
        # Higher rewards for successful exploitation
        if state == 'exploitation' and base_reward > 0.7:
            reward = 2.0
        elif state == 'privilege_escalation' and base_reward > 0.6:
            reward = 3.0
        elif state == 'data_exfiltration':
            reward = 5.0
        else:
            reward = base_reward
        
        done = next_state == 'data_exfiltration'
        
        return next_state, reward, done
    
    async def _generate_synthetic_training_data(self, target: str) -> List[Dict[str, Any]]:
        """Generate synthetic training data for neural network"""
        training_data = []
        
        for i in range(100):
            training_data.append({
                'features': np.random.rand(20).tolist(),
                'label': random.randint(0, 9),
                'success': random.random() > 0.5
            })
        
        return training_data
    
    async def _execute_strategy(self, target: str, strategy: Dict[str, Any]) -> float:
        """Execute a strategy and return reward"""
        await asyncio.sleep(0.01)  # Simulate execution
        
        # Simulate different success rates for different strategies
        if strategy['name'] == 'aggressive':
            base_success = 0.6
        elif strategy['name'] == 'balanced':
            base_success = 0.7
        else:  # stealthy
            base_success = 0.8
        
        return base_success + random.random() * 0.2
    
    def _check_convergence(self, rewards: List[float], window: int = 100) -> bool:
        """Check if Q-learning has converged"""
        if len(rewards) < window:
            return False
        
        recent_rewards = rewards[-window:]
        variance = np.var(recent_rewards)
        
        return variance < 0.1
