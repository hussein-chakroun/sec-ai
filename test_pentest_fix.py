#!/usr/bin/env python3
"""
Test script to verify low context mode pentest fixes
Run this to check if the fallback logic works correctly
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from core.llm_orchestrator import LLMOrchestrator, OpenAIProvider
from loguru import logger

def test_json_extraction():
    """Test the JSON extraction with various malformed responses"""
    # Mock provider for testing
    class MockProvider:
        def generate(self, prompt, system_prompt=None):
            return "irrelevant text"
    
    provider = MockProvider()
    orchestrator = LLMOrchestrator(provider, low_context_mode=True)
    
    test_cases = [
        # Valid JSON
        ('{"tool": "sqlmap", "parameters": {}}', True),
        # JSON in markdown
        ('```json\n{"tool": "hydra", "parameters": {}}\n```', True),
        # JSON with surrounding text
        ('Here is the result: {"tool": "nmap", "parameters": {}} - done', True),
        # Malformed JSON
        ('This is not JSON at all', False),
        # Empty response
        ('', False),
    ]
    
    logger.info("Testing JSON extraction...")
    for response, should_succeed in test_cases:
        result = orchestrator._extract_json_from_response(response)
        success = result is not None
        
        status = "✅" if success == should_succeed else "❌"
        logger.info(f"{status} Response: {response[:50]}... | Expected: {should_succeed} | Got: {success}")
    
    print()

def test_fallback_decisions():
    """Test intelligent fallback decision making"""
    class MockProvider:
        def generate(self, prompt, system_prompt=None):
            return "irrelevant text"
    
    provider = MockProvider()
    orchestrator = LLMOrchestrator(provider, low_context_mode=True)
    
    logger.info("Testing fallback decisions...")
    
    # Test case 1: HTTP service detected
    scan_results = {
        "parsed": {
            "services": [
                {"port": 80, "service": "http", "state": "open"}
            ]
        }
    }
    decision = orchestrator._generate_fallback_decision(scan_results, {})
    logger.info(f"✅ HTTP detected → Tool: {decision['tool']} | Reasoning: {decision['reasoning']}")
    assert decision['tool'] == 'sqlmap', f"Expected sqlmap, got {decision['tool']}"
    
    # Test case 2: SSH service detected
    scan_results = {
        "parsed": {
            "services": [
                {"port": 22, "service": "ssh", "state": "open"}
            ]
        }
    }
    decision = orchestrator._generate_fallback_decision(scan_results, {})
    logger.info(f"✅ SSH detected → Tool: {decision['tool']} | Reasoning: {decision['reasoning']}")
    assert decision['tool'] == 'hydra', f"Expected hydra, got {decision['tool']}"
    
    # Test case 3: No clear services
    scan_results = {"parsed": {}}
    decision = orchestrator._generate_fallback_decision(scan_results, {})
    logger.info(f"✅ No services → Tool: {decision['tool']} | Reasoning: {decision['reasoning']}")
    
    print()

def test_low_context_truncation():
    """Test that large data gets truncated in low context mode"""
    class MockProvider:
        def generate(self, prompt, system_prompt=None):
            # Check if prompt was truncated
            if "truncated" in prompt:
                logger.info("✅ Large data was truncated as expected")
            return '{"tool": "none", "parameters": {}}'
    
    provider = MockProvider()
    orchestrator = LLMOrchestrator(provider, low_context_mode=True, chunk_size=100)
    
    # Create large scan results
    large_results = {"data": "X" * 10000}  # 10KB of data
    
    logger.info("Testing low context mode data truncation...")
    try:
        decision = orchestrator.decide_next_action(large_results, {})
        logger.info("✅ Low context mode handled large data without crashing")
    except Exception as e:
        logger.error(f"❌ Failed to handle large data: {e}")
    
    print()

if __name__ == "__main__":
    logger.info("=== SEC-AI Pentest Fix Verification ===\n")
    
    test_json_extraction()
    test_fallback_decisions()
    test_low_context_truncation()
    
    logger.info("=== All Tests Complete ===")
    logger.info("\nThe fixes should now allow pentests to continue beyond reconnaissance!")
    logger.info("Try running: python main.py --cli --target <your-target>")
