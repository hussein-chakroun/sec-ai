# Pentest Low Context Mode Fix

## Problem

When running pentests in low context mode, the application would generate an nmap report and then immediately stop, never proceeding to exploitation or other phases.

## Root Cause

The issue was in `core/llm_orchestrator.py` in the `decide_next_action()` method:

1. **LLM Response Parsing Failures**: In low context mode, the LLM would often struggle to generate valid JSON due to limited context/memory
2. **Premature "none" Fallback**: When JSON parsing failed, the system immediately returned `"tool": "none"`, signaling to stop
3. **No Retry Logic**: There was no attempt to retry or use intelligent fallbacks
4. **No Context Truncation**: Large scan results weren't being truncated for low context mode

This caused the pentest to conclude after the first reconnaissance phase every time.

## Fixes Applied

### 1. Enhanced JSON Extraction (`_extract_json_from_response`)
Created a new helper method with multiple parsing strategies:
- Direct JSON parsing
- Extract from markdown code blocks
- Find first JSON object in response
- Extract JSON between braces

### 2. Intelligent Fallback Decisions (`_generate_fallback_decision`)
When LLM fails to provide valid JSON after retries, the system now:
- Analyzes scan results to determine next logical step
- Checks for HTTP services → suggests SQLMap
- Checks for SSH → suggests Hydra
- Checks for FTP → suggests Hydra
- Checks for open ports → suggests Metasploit
- Only returns "none" if no clear attack vectors exist

### 3. Retry Logic
The `decide_next_action()` method now:
- Attempts to parse LLM response
- Retries up to 3 times if parsing fails
- Uses intelligent fallback only after all retries exhausted

### 4. Low Context Optimizations
- Truncates large scan results to fit within chunk_size
- Truncates context data to prevent overflow
- Improved prompts specifically for low context scenarios

### 5. Better Error Handling & Logging
In `core/pentest_engine.py`:
- Added warnings when fallback decisions are used
- Shows raw LLM response for debugging
- Tracks consecutive failures
- Stops after 3 consecutive failures to prevent infinite loops
- More detailed logging at each step

### 6. Parameter Format Compatibility
- Handles both dict and list parameter formats
- Converts legacy list formats automatically
- Better error messages and debugging

## Testing

To verify the fix works:

1. Enable low context mode in your config
2. Run a pentest: `python main.py --cli --target <your-target>`
3. Watch the logs - you should see:
   - Initial nmap reconnaissance
   - LLM decision making (or fallback warnings)
   - Subsequent tool executions (SQLMap, Hydra, etc.)
   - Multiple iterations before concluding

## Configuration

If you're still experiencing issues:

1. **Check your LLM model**: Some models are better at JSON generation
   - GPT-4 or Claude: Excellent JSON adherence
   - GPT-3.5: Good but may need retries
   - Local models: May trigger fallbacks more often

2. **Adjust chunk size** in `.env` or GUI:
   ```env
   LOW_CONTEXT_CHUNK_SIZE=2000  # Increase if you have more RAM
   ```

3. **Review logs** for warnings about fallback usage:
   ```
   ⚠️  Using fallback decision due to LLM response parsing issues
   ```

## What to Expect Now

✅ **Before**: Nmap scan → Stop
✅ **After**: Nmap scan → SQLMap/Hydra/Metasploit → Exploitation attempts → Report

The system will now continue through multiple phases even when the LLM struggles with JSON formatting in low context mode.
