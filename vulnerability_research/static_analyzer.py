"""
Static Binary Analysis Module
Analyzes binaries and source code for vulnerabilities
"""

import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
import re

logger = logging.getLogger(__name__)


class StaticAnalyzer:
    """
    Static analysis for binaries and source code
    Identifies potential vulnerabilities without execution
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.vuln_patterns = self._load_vulnerability_patterns()
        
    async def analyze_binary(self, binary_path: str) -> Dict[str, Any]:
        """
        Perform static analysis on a binary
        
        Args:
            binary_path: Path to binary file
            
        Returns:
            Analysis results
        """
        logger.info(f"Static analysis of binary: {binary_path}")
        
        results = {
            'binary': binary_path,
            'vulnerabilities': [],
            'security_features': {},
            'functions': [],
            'imports': []
        }
        
        try:
            # Use radare2, Ghidra, or similar for analysis
            results['security_features'] = await self._check_security_features(binary_path)
            results['functions'] = await self._analyze_functions(binary_path)
            results['imports'] = await self._extract_imports(binary_path)
            results['vulnerabilities'] = await self._find_binary_vulnerabilities(binary_path)
            
            logger.info(f"Found {len(results['vulnerabilities'])} potential issues")
            
            return results
            
        except Exception as e:
            logger.error(f"Binary analysis error: {e}")
            return {'error': str(e), 'vulnerabilities': []}
    
    async def analyze_source(self, source_path: str, language: str = 'auto') -> Dict[str, Any]:
        """
        Perform static analysis on source code
        
        Args:
            source_path: Path to source file or directory
            language: Programming language (auto-detect if not specified)
            
        Returns:
            Analysis results
        """
        logger.info(f"Static analysis of source: {source_path}")
        
        results = {
            'source': source_path,
            'language': language,
            'vulnerabilities': [],
            'code_quality': {},
            'dependencies': []
        }
        
        try:
            # Auto-detect language
            if language == 'auto':
                language = self._detect_language(source_path)
                results['language'] = language
            
            # Scan for vulnerability patterns
            vulns = await self._scan_source_patterns(source_path, language)
            results['vulnerabilities'] = vulns
            
            # Analyze dependencies
            deps = await self._analyze_dependencies(source_path, language)
            results['dependencies'] = deps
            
            # Code quality metrics
            results['code_quality'] = await self._analyze_code_quality(source_path)
            
            logger.info(f"Found {len(vulns)} potential vulnerabilities in source")
            
            return results
            
        except Exception as e:
            logger.error(f"Source analysis error: {e}")
            return {'error': str(e), 'vulnerabilities': []}
    
    async def _check_security_features(self, binary_path: str) -> Dict[str, bool]:
        """Check binary security features"""
        features = {
            'nx': False,
            'pie': False,
            'canary': False,
            'relro': False,
            'stripped': False
        }
        
        try:
            # Use checksec or similar tool
            # This is a simplified check
            
            # For Linux binaries, could use readelf
            # For Windows, could use dumpbin
            
            # Placeholder - would actually parse binary headers
            features['nx'] = True  # Most modern binaries have NX
            
        except Exception as e:
            logger.warning(f"Security feature check failed: {e}")
        
        return features
    
    async def _analyze_functions(self, binary_path: str) -> List[Dict[str, Any]]:
        """Extract and analyze functions"""
        functions = []
        
        try:
            # Use radare2, angr, or similar
            # r2pipe example (if available):
            # import r2pipe
            # r2 = r2pipe.open(binary_path)
            # funcs = r2.cmdj('aflj')  # Get functions as JSON
            
            # Placeholder function analysis
            functions.append({
                'name': 'main',
                'address': '0x401000',
                'size': 256,
                'calls': ['printf', 'scanf'],
                'risk_level': 'medium'
            })
            
        except Exception as e:
            logger.warning(f"Function analysis failed: {e}")
        
        return functions
    
    async def _extract_imports(self, binary_path: str) -> List[str]:
        """Extract imported functions"""
        imports = []
        
        try:
            # Parse import table
            # Would use actual binary parsing here
            dangerous_imports = [
                'system', 'exec', 'strcpy', 'strcat', 'sprintf',
                'gets', 'scanf', 'vsprintf'
            ]
            
            # Placeholder
            imports = ['printf', 'scanf', 'strcpy']
            
        except Exception as e:
            logger.warning(f"Import extraction failed: {e}")
        
        return imports
    
    async def _find_binary_vulnerabilities(self, binary_path: str) -> List[Dict[str, Any]]:
        """Find potential vulnerabilities in binary"""
        vulnerabilities = []
        
        try:
            # Check for dangerous function usage
            imports = await self._extract_imports(binary_path)
            
            dangerous_funcs = {
                'strcpy': 'Buffer overflow risk - no bounds checking',
                'strcat': 'Buffer overflow risk - no bounds checking',
                'sprintf': 'Buffer overflow risk - use snprintf',
                'gets': 'Critical - always unsafe, use fgets',
                'scanf': 'Format string vulnerability risk',
                'system': 'Command injection risk',
                'exec': 'Command injection risk'
            }
            
            for func in imports:
                if func in dangerous_funcs:
                    vulnerabilities.append({
                        'type': 'dangerous_function',
                        'function': func,
                        'severity': 'high' if func in ['gets', 'system'] else 'medium',
                        'description': dangerous_funcs[func]
                    })
            
        except Exception as e:
            logger.warning(f"Vulnerability detection failed: {e}")
        
        return vulnerabilities
    
    def _detect_language(self, path: str) -> str:
        """Detect programming language from file extension"""
        path_obj = Path(path)
        
        ext_map = {
            '.c': 'c',
            '.cpp': 'cpp',
            '.cc': 'cpp',
            '.cxx': 'cpp',
            '.h': 'c',
            '.hpp': 'cpp',
            '.py': 'python',
            '.java': 'java',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.go': 'go',
            '.rs': 'rust',
            '.php': 'php'
        }
        
        if path_obj.is_file():
            return ext_map.get(path_obj.suffix, 'unknown')
        
        return 'unknown'
    
    async def _scan_source_patterns(self, source_path: str, language: str) -> List[Dict[str, Any]]:
        """Scan source code for vulnerability patterns"""
        vulnerabilities = []
        
        try:
            path_obj = Path(source_path)
            
            # Get all source files
            if path_obj.is_dir():
                files = list(path_obj.rglob('*'))
            else:
                files = [path_obj]
            
            for file_path in files:
                if file_path.is_file():
                    file_vulns = await self._scan_file(file_path, language)
                    vulnerabilities.extend(file_vulns)
            
        except Exception as e:
            logger.warning(f"Pattern scanning failed: {e}")
        
        return vulnerabilities
    
    async def _scan_file(self, file_path: Path, language: str) -> List[Dict[str, Any]]:
        """Scan individual file for vulnerabilities"""
        vulnerabilities = []
        
        try:
            content = file_path.read_text(errors='ignore')
            lines = content.split('\n')
            
            # Get patterns for this language
            patterns = self.vuln_patterns.get(language, {})
            
            for line_num, line in enumerate(lines, 1):
                for pattern_name, pattern_info in patterns.items():
                    if re.search(pattern_info['regex'], line):
                        vulnerabilities.append({
                            'type': pattern_name,
                            'file': str(file_path),
                            'line': line_num,
                            'code': line.strip(),
                            'severity': pattern_info['severity'],
                            'description': pattern_info['description']
                        })
            
        except Exception as e:
            logger.debug(f"File scan failed for {file_path}: {e}")
        
        return vulnerabilities
    
    def _load_vulnerability_patterns(self) -> Dict[str, Dict[str, Any]]:
        """Load vulnerability detection patterns"""
        patterns = {
            'c': {
                'buffer_overflow': {
                    'regex': r'\b(strcpy|strcat|sprintf|gets)\s*\(',
                    'severity': 'high',
                    'description': 'Unsafe function - buffer overflow risk'
                },
                'format_string': {
                    'regex': r'\b(printf|sprintf|fprintf)\s*\([^,)]*\)',
                    'severity': 'medium',
                    'description': 'Potential format string vulnerability'
                },
                'integer_overflow': {
                    'regex': r'malloc\s*\(\s*\w+\s*\*\s*\w+',
                    'severity': 'medium',
                    'description': 'Potential integer overflow in allocation'
                }
            },
            'python': {
                'command_injection': {
                    'regex': r'\b(os\.system|subprocess\.call|eval|exec)\s*\(',
                    'severity': 'high',
                    'description': 'Command injection risk'
                },
                'sql_injection': {
                    'regex': r'execute\s*\([^?]*%[^)]*\)',
                    'severity': 'high',
                    'description': 'SQL injection risk - use parameterized queries'
                },
                'deserialization': {
                    'regex': r'\bpickle\.loads?\s*\(',
                    'severity': 'high',
                    'description': 'Unsafe deserialization'
                }
            },
            'javascript': {
                'xss': {
                    'regex': r'\.innerHTML\s*=|document\.write\s*\(',
                    'severity': 'high',
                    'description': 'XSS vulnerability risk'
                },
                'eval': {
                    'regex': r'\beval\s*\(',
                    'severity': 'high',
                    'description': 'Code injection via eval'
                }
            }
        }
        
        return patterns
    
    async def _analyze_dependencies(self, source_path: str, language: str) -> List[Dict[str, Any]]:
        """Analyze project dependencies for known vulnerabilities"""
        dependencies = []
        
        # Check various dependency files
        dep_files = {
            'python': ['requirements.txt', 'Pipfile', 'setup.py'],
            'javascript': ['package.json'],
            'java': ['pom.xml', 'build.gradle'],
            'go': ['go.mod']
        }
        
        files_to_check = dep_files.get(language, [])
        
        for dep_file in files_to_check:
            dep_path = Path(source_path) / dep_file
            if dep_path.exists():
                deps = await self._parse_dependencies(dep_path, language)
                dependencies.extend(deps)
        
        return dependencies
    
    async def _parse_dependencies(self, dep_file: Path, language: str) -> List[Dict[str, Any]]:
        """Parse dependency file"""
        dependencies = []
        
        try:
            content = dep_file.read_text()
            
            if language == 'python' and dep_file.name == 'requirements.txt':
                for line in content.split('\n'):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # Parse package==version format
                        parts = re.split(r'[=<>!]', line)
                        if parts:
                            dependencies.append({
                                'name': parts[0].strip(),
                                'type': 'python',
                                'source': str(dep_file)
                            })
            
        except Exception as e:
            logger.debug(f"Dependency parsing failed: {e}")
        
        return dependencies
    
    async def _analyze_code_quality(self, source_path: str) -> Dict[str, Any]:
        """Analyze code quality metrics"""
        metrics = {
            'complexity': 0,
            'lines_of_code': 0,
            'comment_ratio': 0.0
        }
        
        try:
            path_obj = Path(source_path)
            
            if path_obj.is_file():
                files = [path_obj]
            else:
                files = list(path_obj.rglob('*'))
            
            total_lines = 0
            comment_lines = 0
            
            for file_path in files:
                if file_path.is_file():
                    try:
                        content = file_path.read_text(errors='ignore')
                        lines = content.split('\n')
                        total_lines += len(lines)
                        
                        # Count comments (simplified)
                        for line in lines:
                            stripped = line.strip()
                            if stripped.startswith('//') or stripped.startswith('#'):
                                comment_lines += 1
                    except:
                        pass
            
            metrics['lines_of_code'] = total_lines
            metrics['comment_ratio'] = comment_lines / max(total_lines, 1)
            
        except Exception as e:
            logger.debug(f"Code quality analysis failed: {e}")
        
        return metrics
